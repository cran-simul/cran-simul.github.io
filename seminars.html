<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>seminars | SiMul Research group @ CRAN</title> <meta name="author" content="SiMul Research group @ CRAN"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://cran-simul.github.io/seminars"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">SiMul </span>Research group @ CRAN</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/jobs/">jobs</a> </li> <li class="nav-item active"> <a class="nav-link" href="/seminars">seminars<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/team">team</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">seminars</h1> <p class="post-description"></p> </header> <article> <p>Our group host seminars on a regular monthly basis, either in person or remotely. To get the latest anouncements, you can subscribe to the seminar mailing list <a href="mailto:cran-simul-seminars@univ-lorraine.fr">cran-simul-seminars@univ-lorraine.fr</a> as follows:</p> <ul> <li>to subscribe, send a blank email to <a href="mailto:sympa@univ-lorraine.fr">sympa@univ-lorraine.fr</a> with object <strong>SUBSCRIBE cran-simul-seminars</strong>. A admin will review and validate your request.</li> <li>to unsubscribe, send a blank email to <a href="mailto:sympa@univ-lorraine.fr">sympa@univ-lorraine.fr</a> with object <strong>UNSUBSCRIBE cran-simul-seminars</strong>. A admin will review and validate your request.</li> </ul> <h2 id="upcoming-seminars">Upcoming seminars</h2> <p><b>Linus Bleisten</b> (Inria Paris)<br> March, 11th 2024, 14h-15h<br> <em>TBA</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-up-1" aria-expanded="false" aria-controls="collapse-u-1"> Abstract </button> </p> <div class="collapse" id="collapse-up-1"> <div class="card card-body"> TBA </div> </div> <h2 id="past-seminars">Past seminars</h2> <h2 class="year">2023</h2> <p><b>Xavier Luciani</b> (University of Toulon)<br> October, 5th 2023, 13h-14h<br> <em>Décomposition Canonique Polyadique et Diagonalisation Conjointe de matrices par Similitude : algorithmes et applications</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-1" aria-expanded="false" aria-controls="collapse-past-1"> Abstract </button> </p> <div class="collapse" id="collapse-past-1"> <div class="card card-body"> En traitement du signal, la Décomposition Canonique Polyadique (DCP) consiste à décomposer un tableau multidimensionnel (appelé ici tenseur) en une combinaison multilinéaire d'un minimum de facteurs, comprenant généralement les signaux d'intérêt. Cette approche est donc couramment utilisée en séparation de sources, identification de mélange et plus généralement pour la résolution de problèmes inverses. Par ailleurs, de nombreux liens ont été établis entre la DCP et le problème de diagonalisation conjointe de matrices, que l'on retrouve également au cœur de nombreuses méthodes de séparation de sources. Nous montrerons dans la première partie de cette présentation comment réécrire la DCP sous la forme d'une Diagonalisation Conjointe de matrices par Similitude (DCS) afin d'en dériver un algorithme de calcul efficace. Nous présenterons alors plusieurs familles d'algorithmes de DCS permettant notamment de traiter le cas de signaux à valeurs complexes ou de tenir compte de contraintes de non négativité. Dans la seconde partie, nous nous placerons dans le contexte applicatif de la spectroscopie de fluorescence pour introduire un autre algorithme de DCP permettant cette fois la mise à jour des facteurs de la décomposition (et de leur nombre) au fil de l'acquisition de nouveaux signaux. Nous verrons que ces deux algorithmes très différents dans leur principe ont pour point commun une certaine résistance à la surestimation du nombre de facteurs de la DCP. Enfin nous conclurons cette présentation en évoquant nos travaux actuels en collaboration avec le CRAN et consistant à étendre les notions de DCP et de DCS aux tenseurs et matrices dont les éléments appartiennent à l'algèbre des quaternions. </div> </div> <p><b>Nuha Diab</b> (Tel Aviv University, Israel)<br> September, 26th 2023, 14h-14h40<br> <em>Optimal super-resolution of close point sources and stability of Prony’s method</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-2" aria-expanded="false" aria-controls="collapse-past-2"> Abstract </button> </p> <div class="collapse" id="collapse-past-2"> <div class="card card-body"> We consider the problem of recovering a linear combination of Dirac masses from noisy Fourier samples, also known as the problem of super-resolution. Following recent derivation of min-max bounds for this problem when some of the sources collide, we develop an optimal algorithm which provably achieves these bounds in such a challenging scenario. Our method is based on the well-known Prony's method for exponential fitting, and a novel analysis of its stability in the near-colliding regime, combined with the decimation technique for improving the conditioning of the problem. </div> </div> <p><b>Joppe De Jonghe</b> (KU Leuven, Belgium)<br> April, 6th 2023, 14h-15h<br> <em>Learning non-linearities in the two layer decoupling problem with an application to neural network compression</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-3" aria-expanded="false" aria-controls="collapse-past-3"> Abstract </button> </p> <div class="collapse" id="collapse-past-3"> <div class="card card-body"> Methods for the decoupling of multivariate functions have been developed in order to determine the parameters and internal representations of non-linear static components in block-oriented system identification. These methods solve the single layer decoupling problem, whose solution has a natural interpretation as a neural network with a single hidden layer with flexible activation functions in the neurons. As a result, these methods have been used to compress neural (sub)networks. However, currently only compression to a single hidden layer is well understood but more complex (sub)networks may require more flexibility in the number of hidden layers. Providing compression to more than one hidden layer corresponds to approximating a solution of a multi-layer decoupling problem. In this talk I will shortly describe the single layer decoupling problem and why multi-layer decoupling is relevant, more specifically forneural network compression. Next, I will present the two layer decoupling problem as well as a solution strategy. In addition, two algorithms for approximating a solution will be discussed and described conceptually. </div> </div> <h2 class="year">2022</h2> <p><b>Khazhgali Kozhasov</b> (TU Braunschweig, Germany)<br> December, 6th 2022, 14h-15h<br> <em>Real aspects of the problem of rank-one approximation</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-1" aria-expanded="false" aria-controls="collapse-past-1"> Abstract </button> </p> <div class="collapse" id="collapse-past-1"> <div class="card card-body"> Let us consider the problem of approximating a real tensor T (of a given format) by a rank-one tensor T_1 that minimizes (the Frobenius) norm ||T-S_1|| among all rank-one tensors S_1. A tensor T_1 is, in particular, a critical point of the square of the distance function dist_T: S_1 -&gt; ||T-S_1||^2 on the manifold X of rank-one tensors. The largest possible number N of critical points of dist_T among all generic T can be interpreted as a measure of complexity of the rank-one approximation problem. I will discuss a bound on N due to Friedland and Ottaviani and will explain a technique that has been successfully used to determine a sharp bound on the number of symmetric critical points of dist_T for a symmetric tensor T. If time permits, I will discuss tensors that have worst rank-one approximation error and mention a recent result that roughly means that symmetric tensors are as far from being of rank one as general tensors. </div> </div> <p><b>Lorena León</b> (IRIT, University of Toulouse)<br> November, 17th 2022, 14h-15h<br> <em>Bayesian Multivariate Multifractal Analysis with application to Drowsiness Detection</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-2" aria-expanded="false" aria-controls="collapse-past-2"> Abstract </button> </p> <div class="collapse" id="collapse-past-2"> <div class="card card-body"> Multifractal analysis has become a reference tool for signal and image processing. Grounded in the quantification of local regularity fluctuations, it has proven useful in an increasing range of applications, yet so far involving only univariate data (scalar valued time series or single channel images). Recently the theoretical ground for multivariate multifractal analysis has been devised, showing potential for quantifying transient higher-order dependence beyond linear correlation among collections of data. However, the accurate estimation of the parameters associated with a multivariate multifractal model remains challenging, especially for small sample size data. This work studies an original Bayesian framework for multivariate multifractal estimation, combining a novel and generic multivariate statistical model, a Whittle-based likelihood approximation and a data augmentation strategy allowing parameter separability. This careful design enables efficient estimation procedures to be constructed for two relevant choices of priors using a Gibbs sampling strategy. Monte Carlo simulations, conducted on synthetic multivariate signals and images with various sample sizes and multifractal parameter settings, demonstrate significant performance improvements over the state of the art, at only moderately larger computational cost. Moreover, we show the relevance of the proposed framework for real-world data modeling in the important application of drowsiness detection from multichannel physiological signals. </div> </div> <p><b>Jonathan Gillard</b> (Cardiff University)<br> November, 10th 2022, 14h-15h<br> <em>Low-rank methods for time series analysis.</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-3" aria-expanded="false" aria-controls="collapse-past-3"> Abstract </button> </p> <div class="collapse" id="collapse-past-3"> <div class="card card-body"> This talk will describe some classic and recent results on low-rank methods for problems of time series analysis. The first typical and fundamental step to enable low-rank methods in this setting is to embed a time series into a Hankel matrix; low-rank approximations of this matrix which maintain the Hankel structure have meaning for classical problems such as approximation, de-noising, and forecasting. This claim will be justified in the talk and is part of a broader field known as structured low-rank approximation (SLRA). We discuss some results in SLRA before describing recent work on the nuclear norm convex relaxation of the rank minimization of Hankel matrices for forecasting, which gives rise to interesting theory and much potential for application, and can be viewed as a particular problem of matrix completion. </div> </div> <p><b>Barbara Pascal</b> (CRIStAL, Lille)<br> July, 12th 2022, 10h30-11h30<br> <em>The Kravchuk transform: a novel covariant representation for discrete signals amenable to zero-based detection tests.</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-4" aria-expanded="false" aria-controls="collapse-past-4"> Abstract </button> </p> <div class="collapse" id="collapse-past-4"> <div class="card card-body"> Recent works in time-frequency analysis proposed to switch the focus from the maxima of the spectrogram toward its zeros, which form a random point pattern with a very stable structure. Several signal processing tasks, such as component disentanglement and signal detection procedures, have already been renewed by using modern spatial statistics onthe pattern of zeros. Tough, they require cautious choice of both the discretization strategy and the observation window in the time-frequency plane. To overcome these limitations, we propose a generalized time-frequency representation: the Kravchuk transform, especially designed for discrete signals analysis, whose phase space is the unit sphere, particularly amenable to spatial statistics. We show that it has all desired properties for signal processing, among which covariance, invertibility and symmetry, and that the point process of the zeros of the Kravchuk transform of complex white Gaussian noise coincides with the zeros of the spherical Gaussian Analytic Function. Elaborating on this theorem, we finally develop a Monte Carlo envelope test procedure for signal detection based on the spatial statistics of the zeros of the Kravchuk spectrogram. After reviewing the unorthodox path focusing on the zeros of the standard spectrogram and the associated theoretical results on the distribution of zeros in the case of white noise, I will introduce the Kravchuk transform and study the random point process of its zeros from a spatial statistics perspective. Then I will present the designed Monte Carlo envelop test, and illustrate its numerical performance in adversarial settings, with both low signal-to-noise ratio and small number of samples, and compare it to state-of-the-art zeros-based detection procedures. </div> </div> <p><b>Tulay Adali</b> (University of Maryland, Baltimore County)<br> May, 30th 2022, 13h30-14h30<br> <em>Independent Component and Vector Analyses</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-5" aria-expanded="false" aria-controls="collapse-past-5"> Abstract </button> </p> <div class="collapse" id="collapse-past-5"> <div class="card card-body"> In many fields today, such as neuroscience, remote sensing, computational social science, and physical sciences, multiple sets of data are readily available. Matrix and tensor factorizations enable joint analysis, i.e., fusion, of these multiple datasets such that they can fully interact and inform each other while also minimizing the assumptions placed on their inherent relationships. A key advantage of these methods is the direct interpretability of their results. This talk presents an overview of models based on independent component analysis (ICA), and its generalization to multiple datasets, independent vector analysis (IVA) with examples in fusion of neuroimaging data. Relationship of IVA to other methods such as multiset canonical correlation analysis (MCCA) is discussed, and a number of important directions of research are addressed, along with the challenges. </div> </div> <p><b>Raphael Mignot</b> (IECL, Nancy)<br> May, 05th 2022, 10h30-11h30<br> <em>Barycentres de séries temporelles : une nouvelle approche basée sur la méthode de la signature</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-6" aria-expanded="false" aria-controls="collapse-past-6"> Abstract </button> </p> <div class="collapse" id="collapse-past-6"> <div class="card card-body"> La méthode de la signature a été largement utilisée pour l'analyse des séries temporelles multivariées. Cette approche a prouvé son efficacité pour de nombreuses applications en apprentissage statistique. La définition d'une notion de barycentre dans l'espace des signatures est un premier pas prometteur permettant de développer de nouvelles extensions de l'analyse en composantes principales (ACP) ou de l'algorithme des k-moyennes aux séries temporelles. </div> </div> <p><b>Rima Khouja</b> (INRIA, Sophia Antipolis)<br> April, 28th 2022, 10h-11h<br> <em>Riemannian Newton optimization methods for the symmetric tensor approximation problem</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-7" aria-expanded="false" aria-controls="collapse-past-7"> Abstract </button> </p> <div class="collapse" id="collapse-past-7"> <div class="card card-body"> Tensors are higher order generalization of matrices. They appear in a myriad of applications. The tensor rank decomposition is to write the tensor as a minimal sum of simple rank-1 tensors. In practice, the presence of noise in the tensor's inputs means that computing an approximated low rank decomposition is more relevant than computing the exact tensor rank decomposition. This problem is known as the low rank tensor approximation problem. In this talk, we discuss the low rank tensor approximation problem for symmetric tensors i.e. tensors with unchanged entries under any permutation of their indices. The symmetric tensors are considered with complex coefficients. We present a Riemannian optimization approach proposing Riemannian Newton and Riemannian Gauss-Newton algorithms to solve this problem. We show how the low rank symmetric tensor approximation problem can be used for tackling the problem of recovering spherical Gaussian mixture models from datasets, where the tensor is built from empirical moments of the data distribution. </div> </div> <h2 class="year">2021</h2> <p><b>Clément Elvira</b> (IETR, Rennes)<br> December, 9th 2021, 10h-11h<br> <em>Safe screening: introduction and perspectives</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-1" aria-expanded="false" aria-controls="collapse-past-1"> Abstract </button> <button type="button" class="btn btn-light" onclick="window.location='/assets/seminars/2021_12_Elvira.pdf';">slides</button> </p> <div class="collapse" id="collapse-past-1"> <div class="card card-body"> </div> </div> <p><b>Simon Barthelmé</b> (Gipsa-Lab, France)<br> November, 19th 2021, 15h-16h<br> <em>Smoothing (Large) Graph Signals using Random Forests</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-2" aria-expanded="false" aria-controls="collapse-past-2"> Abstract </button> </p> <div class="collapse" id="collapse-past-2"> <div class="card card-body"> A natural way of denoising graph signals is to penalise local variation using the graph Laplacian. Because this has worst-case cost O(n^3) in the number of nodes, exact methods cannot be used for very large graphs. I'll show how a simple stochastic process can be used to obtain fast unbiased estimators for the smoothed signal. I'll introduce some variance reduction techniques, including a gradient-descent technique that works more generally whenever an unbiased estimator of a least-squares problem is available. Joint work with Yigit Pilavci, Nicolas Tremblay, P-O Amblard </div> </div> <p><b>Mariya Ishteva</b> (KU Leuven, Belgium)<br> October, 21th 2021, 14h30-15h<br> <em>Tensor methods with applications in system identification</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-3" aria-expanded="false" aria-controls="collapse-past-3"> Abstract </button> <button type="button" class="btn btn-light" onclick="window.location='/assets/seminars/1_Ishteva_Presentation_Mariya_2021_Nancy.pdf';">slides</button> </p> <div class="collapse" id="collapse-past-3"> <div class="card card-body"> TBD </div> </div> <p><b>Jean-Yves Tourneret</b> (ENSEEIHT, Toulouse)<br> October, 21th 2021, 15h-15h40<br> <em>Hypersphere Fitting: Model, Algorithms and Future Work</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-4" aria-expanded="false" aria-controls="collapse-past-4"> Abstract </button> <button type="button" class="btn btn-light" onclick="window.location='/assets/seminars/2_Tourneret_Slides_Nancy_21octobre2021.pdf';">slides</button> </p> <div class="collapse" id="collapse-past-4"> <div class="card card-body"> We will present a recent EM algorithm for hypersphere fitting based on a von Mises-Fisher prior. The algorithm achieves competitive performance compared to the state-of-the-art. In addition, it can be easily robustified to mitigate the presence of potential outliers. After presenting some results obtained with this algorithm, we will discuss some open issues related to the application to LiDAR point clouds. These open issues include the consideration of mixtures of hyperspheres, the segmentation and denoising of LiDAR point clouds and the fusion of point clouds with RGB images </div> </div> <p><b>Eric Chaumette</b> (ISAE-Supaéro, Toulouse)<br> October, 21th 2021, 15h40-16h10<br> <em>Robust Linearly Constrained Filtering and Smoothing: Results and Applications</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-5" aria-expanded="false" aria-controls="collapse-past-5"> Abstract </button> <button type="button" class="btn btn-light" onclick="window.location='/assets/seminars/3_Chaumette_Slides_Robust_LCKF_CRAN.pdf';">slides</button> </p> <div class="collapse" id="collapse-past-5"> <div class="card card-body"> It is well-known that Wiener and Kalman filter (KF) like techniques are sensitive to misspecified covariances, uncertainties in the system matrices and parameters, filter initialization or unexpected system behaviors induced by time-varying environments, harsh propagation conditions, malicious interferences or unmodeled inputs. In this talk we introduce a possible solution to robustify these estimation techniques through linear constraints (LCs): i) we detail the linearly constrained KF (LCKF), where a set of non-stationary LCs can be set at every time step, ii) we show how to use such LCs to mitigate modeling errors in general mismatched linear discrete state-space models, and iii) we point the reader to some recent LCKF extensions (i.e., information filter, invariant filter, linear smoother and LC-extended/cubature-KF). Some applications of interest are provided to support the discussion: robust array processing, GNSS position and attitude estimation, invariant navigation and visual SLAM. </div> </div> <p><b>Radu Ranta</b> (CRAN, Université de Lorraine)<br> June, 24th 2021, 10h-11h<br> <em>Low-Rank Inverse Problems in Brain Signal Processing</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-6" aria-expanded="false" aria-controls="collapse-past-6"> Abstract </button> </p> <div class="collapse" id="collapse-past-6"> <div class="card card-body"> The presentation will start by introducing the basics principles of biophysics allowing to model the electrophysiological brain measurements (EEG / SEEG / micro-electrodes), and more precisely the relations between the neural current sources and the electrodes. Once the signal model defined, I will briefly present some of the classical methods for solving the inverse problem of brain sources estimation (localization and activity), and I will focus next on our work on sparse approximations and low-rank (exact and approximate) source estimates. </div> </div> <p><b>Gaëtan Frusque</b> (ETH Zürich)<br> April, 16th 2021, 10h-11h<br> <em>Inférence et décomposition de graphes dynamiques en neurosciences</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-7" aria-expanded="false" aria-controls="collapse-past-7"> Abstract </button> </p> <div class="collapse" id="collapse-past-7"> <div class="card card-body"> Dynamic graphs make it possible to understand the evolution of complex systems which evolve through time. In this thesis, we look at their applications to understand one of the most common neurological disorder in the world, affecting around 1% of the population: epilepsy. A complete and objective characterization of the patient-specific dynamic graph describing this pathology is crucial for optimal surgical treatment. First, we propose to modify a measure of functional connectivity, the Phase-Locking-Value, in order to infer robust dynamic graph from the neurophysiological signal recorded during an epileptic seizure. Constrained matrix decomposition method is applied to extract the principal features from the dynamic graph describing the pathology. Finally, a clinical study is performed to compare the obtained features from the visual interpretation of a clinician specialized in neurophysiological signal interpretation. </div> </div> <p><b>Titouan Parcollet</b> (Université d’Avignon)<br> March, 24th 2021, 14h-15h<br> <em>Should we use quaternion neural networks? Recent advances and limitations.</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-8" aria-expanded="false" aria-controls="collapse-past-8"> Abstract </button> </p> <div class="collapse" id="collapse-past-8"> <div class="card card-body"> Real-world data used to train modern artificial neural networks reflect the complexity of the environment that we are evolving in. As a consequence, they are neither flat, nor decorrelated nor one dimensional. Instead, scientists have to deal with composed and multidimensional entities, that are characterized by multiple related components, such as color channels describing a single color pixel of an image, or the 3D coordinates of a point denoting the position of a robot. Surprisingly, recent advances on deep learning are mainly focused on developing novel architectures to extract even more relevant and robust high level representations from the input features, while the latter are still poorly considered at a lower and basic level, by being processed with one dimensional real-valued neural models. Neural networks based on complex and quaternion numbers have been used sparsely for many decades. Nonetheless, due to new statements and proofs about the benefits of these models over real-valued ones on many real-world tasks, quaternion based neural networks have been increasingly employed, and novel quaternion based architectures have been proposed. This talk will detail quaternion neural networks architectures for artificial intelligence related tasks, such as image processing, or speech recognition, by introducing first the basics of quaternion numbers, and then describing recent advances on quaternion neural networks with the quaternion convolutional (Interspeech 2018, ICASSP 2019) and recurrent neural networks (ICLR 2019, Interspeech 2020). This presentation will also show their benefits in terms of performances obtained in different tasks, as well as in terms of neural parameters required for learning. Finally, the talk will outline important future research directions to turn quaternion neural networks into a mandatory alternative to real-valued models for real-world tasks. </div> </div> <p><b>Konstantin Usevich</b> (CRAN, SiMul)<br> February, 05th 2021, 10h-11h<br> <em>Kernel matrices in the flat limit</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-9" aria-expanded="false" aria-controls="collapse-past-9"> Abstract </button> </p> <div class="collapse" id="collapse-past-9"> <div class="card card-body"> Kernel matrices are ubiquitous in statistics and machine learning, where they occur most often as covariance matrices of Gaussian processes, in non-parametric or semi-parametric models. In approximation theory, they appear, for example, in approximation and interpolation with radial basis functions. Most of the theoretical work on kernel methods has focused on a large-n asymptotics, characterising the behaviour of kernel matrices as the amount of data increases. Fixed-sample analysis is much more difficult outside of simple cases, such as locations on a regular grid. In this talk I will describe a fixed-sample analysis that was first studied in the context of approximation theory by Fornberg &amp; Driscoll (2002), called the “flat limit”. In flat-limit asymptotics, the goal is to characterise kernel methods as the length-scale of the kernel function tends to infinity, so that kernels appear flat over the range of the data. While the resulting kernel matrix becomes singular, fascinatingly, the interpolation and regression problems remain well-defined in the limit. I the talk, I will mainly report recent results on spectral properties of kernel matrices ( https://arxiv.org/abs/1910.14067). In the flat limit, different types of kernels behave differently, and what matters most is the smoothness of kernel functions. The flat limit also highlights the close kinship between kernel methods and polynomial and spline regression. If time permits, I will discuss some implications for GP regression and Determinantal Point Processes. This is joint work with S. Barthelmé, N. Tremblay and P.-O. Amblard (GIPSA-lab, Grenoble). </div> </div> <p><b>Fateme Ghayem</b> (Gipsa-Lab, Grenoble)<br> January, 13th 2021, 10h-11h<br> <em>Optimal sensor placement for signal extraction</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-10" aria-expanded="false" aria-controls="collapse-past-10"> Abstract </button> </p> <div class="collapse" id="collapse-past-10"> <div class="card card-body"> Many signal processing problems can be cast from a generic setting where a source signal propagates through a given environment to some sensors. Under this setting, we can be interested either in (i) estimating the source signal, or (ii) the environment, or even (iii) the resulting field of signals in some regions of the environment. In all these cases, signals are recorded by multiple sensors located at different positions. Due to price, energy, or ergonomic constraints, the number of sensors is often limited and it becomes crucial to place a few sensors at positions that contain the maximum information. This problem corresponds to optimal sensor placement and it appears in a great number of applications. The way to tackle the problem of optimal sensor placement depends on which of the three aspects mentioned above we want to address. In this talk, we focus on estimating a source signal from a set of noisy measurements collected from a limited number of sensors, and we present new criteria as well as algorithms. Specifically, our first proposed criterion maximizes the average signal to noise ratio (SNR) of the estimated signal, and we experimentally show that the performance obtained by this criterion outperforms the results obtained using classical Kriging-based methods. Since the SNR is uncertain in this context, to achieve a robust signal extraction, we propose a second placement criterion based on the maximization of the probability that the output SNR exceeds a given threshold. This criterion can be easily evaluated using a Gaussian process assumption for the signal, the noise, and the environment. Moreover, to reduce the computational complexity of the joint maximization of the criterion with respect to all sensor positions, we propose a greedy algorithm where the sensor positions are sequentially (i.e. one by one) selected. Finally, for improving the sub-optimal greedy algorithm, we present an optimization approach to locate all the sensors at once. For this purpose, we add a constraint to the problem that can control the average distances between the sensors. To solve our problem, we use an alternating optimization penalty method. </div> </div> <h2 class="year">2020</h2> <p><b>Nikola Besic</b> (Centre Météorologie Radar de Météo-France, Toulouse)<br> December, 14th 2020, 14h-15h<br> <em>Mes expériences dans la télédétection radar : la Terre depuis le ciel, le ciel depuis la Terre, et comment profiter des deux ?</em><br></p> <p> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-1" aria-expanded="false" aria-controls="collapse-past-1"> Abstract </button> </p> <div class="collapse" id="collapse-past-1"> <div class="card card-body"> La télédétection radar est une discipline qui repose sur le traitement du signal, d'images et de données, et sur la physique. Caractérisée par les nombreuses spécificités par rapport aux autres moyens de télédétection, cette discipline s'est montrée indispensable dans l'observation de la Terre et de l'atmosphère. De plus, il s'agit d'un domaine qui a motivé et permis de nombreux travaux de recherche concernant l'analyse statistique du signal, d'images et de données. Nikola Besic partagera avec nous certaines de ces expériences dans l'observation de la Terre depuis le ciel, et dans l'observation du ciel depuis la Terre, au moyen de radar. La première partie de son exposé adresse alors les travaux effectués sur le sujet du Radar à Synthèse d'Ouverture, satellitaire et polarimétrique, qui incluent la problématique des modèles statistiques et de la décomposition, ainsi que l'application dans le contexte des études de la cryosphère. La deuxième partie concerne plutôt l'observation de l'atmosphère avec un radar toujours polarimétrique, mais cette fois-ci terrestre, et ses efforts de trouver un compromis entre la physique et l'apprentissage automatique à partir de données, dans un contexte des méthodes de classification semi-supervisée. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 SiMul Research group @ CRAN. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>