<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>seminars | SiMul Research group @ CRAN</title> <meta name="author" content="SiMul Research group @ CRAN"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://cran-simul.github.io/seminars"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">SiMul </span>Research group @ CRAN</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/jobs/">jobs</a> </li> <li class="nav-item active"> <a class="nav-link" href="/seminars">seminars<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/team">team</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research</a> </li> <li class="nav-item"><a class="nav-link" href="https://cran-simul.github.io/workshop-lorainne-2024/" target="_blank">LoRAINNe’24</a></li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">seminars</h1> <p class="post-description"></p> </header> <article> <p>Our group host seminars on a regular monthly basis, either in person or remotely. To get the latest anouncements, you can subscribe to the seminar mailing list <a href="mailto:cran-simul-seminars@univ-lorraine.fr">cran-simul-seminars@univ-lorraine.fr</a> as follows:</p> <ul> <li>to subscribe, send a blank email to <a href="mailto:sympa@univ-lorraine.fr">sympa@univ-lorraine.fr</a> with object <strong>SUBSCRIBE cran-simul-seminars</strong>. A admin will review and validate your request.</li> <li>to unsubscribe, send a blank email to <a href="mailto:sympa@univ-lorraine.fr">sympa@univ-lorraine.fr</a> with object <strong>UNSUBSCRIBE cran-simul-seminars</strong>. A admin will review and validate your request.</li> </ul> <h2 id="calendar">Calendar</h2> <p>The calendar below includes internal team meetings (GT SiMul) and seminars from invited speakers.</p> <iframe src="https://calendar.google.com/calendar/embed?height=300&amp;wkst=2&amp;ctz=Europe%2FParis&amp;bgcolor=%23ffffff&amp;mode=AGENDA&amp;showTabs=0&amp;showPrint=0&amp;showNav=0&amp;showCalendars=0&amp;src=cGE0OXNwZzAzbmNyNDRsamJxNTY2aWZoM29AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&amp;color=%23E67C73" style="border:solid 1px #777" width="800" height="300" frameborder="0" scrolling="no"></iframe> <h2 id="upcoming-seminars">Upcoming seminars</h2> <div style="margin-top:20px; margin-bottom:20px;"> <b>Leonardo Tomazeli Duarte</b> (University of Campinas (Unicamp, Brazil))<br> January, 27th 2025, 10h30-11h30<br> <em>Fairness-Aware Unsupervised Learning: Contributions to PCA and Data Aggregation</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-upcoming-1" aria-expanded="false" aria-controls="collapse-upcoming-1"> Abstract </button> <div class="collapse" id="collapse-upcoming-1"> <div class="card card-body"> In this talk, we address fairness issues in unsupervised learning and data aggregation. First, we show that classical principal component analysis (PCA) may induce disparities in reconstruction errors across sensitive groups, leading to biased representations. A simple fairness-aware PCA approach is presented, based on a one-dimensional search that exploits the closed-form solution of PCA, significantly reducing group disparities with minimal loss in overall performance. Second, we discuss fairness in multicriteria decision analysis, introducing a framework for adjusting aggregation operators to produce fairer rankings. The approach relies on optimizing a fairness metric for both additive weighting and Choquet integral models, and its effectiveness is demonstrated on synthetic and real datasets. </div> </div> <br> </div> <h2 id="past-seminars">Past seminars</h2> <ul class="nav nav-tabs" id="myTab" role="tablist"> <li class="nav-item"> <a class="nav-link active" id="tab-2025" data-toggle="tab" href="#year-2025" role="tab" aria-controls="year-2025" aria-selected="true">2025</a> </li> <li class="nav-item"> <a class="nav-link " id="tab-2024" data-toggle="tab" href="#year-2024" role="tab" aria-controls="year-2024" aria-selected="false">2024</a> </li> <li class="nav-item"> <a class="nav-link " id="tab-2023" data-toggle="tab" href="#year-2023" role="tab" aria-controls="year-2023" aria-selected="false">2023</a> </li> <li class="nav-item"> <a class="nav-link " id="tab-2022" data-toggle="tab" href="#year-2022" role="tab" aria-controls="year-2022" aria-selected="false">2022</a> </li> <li class="nav-item"> <a class="nav-link " id="tab-2021" data-toggle="tab" href="#year-2021" role="tab" aria-controls="year-2021" aria-selected="false">2021</a> </li> <li class="nav-item"> <a class="nav-link " id="tab-2020" data-toggle="tab" href="#year-2020" role="tab" aria-controls="year-2020" aria-selected="false">2020</a> </li> </ul> <div class="tab-content" id="myTabContent"> <div class="tab-pane fade show active" id="year-2025" role="tabpanel" aria-labelledby="tab-2025"> <br> <b>Pavlo Mozharovskyi</b> (Télécom Paris, France)<br> October, 10th 2025, 14h00-15h00<br> <em>Explainable anomaly detection using data depth</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-1" aria-expanded="false" aria-controls="collapse-past-1"> Abstract </button> <div class="collapse" id="collapse-past-1"> <div class="card card-body"> Anomaly detection is a branch of data analysis and machine learning which aims at identifying observations that exhibit abnormal behaviour. Be it measurement errors, disease development, severe weather, production quality default(s) (items) or failed equipment, financial frauds or crisis events, their on-time identification, isolation and explanation constitute an important task in almost any branch of science and industry. By providing a robust ordering, data depth - statistical function that measures belongingness of any point of the space to a data set - becomes a particularly useful tool for detection of anomalies. Already known for its theoretical properties, data depth has undergone substantial computational developments in the last decade and particularly recent years, which has made it applicable for contemporary-sized problems of data analysis and machine learning. We study data depth as an efficient anomaly detection tool, assigning abnormality labels to observations with lower depth values, in a multivariate setting. Practical questions of necessity and reasonability of invariances and shape of the depth function, their robustness and computational complexity, choice of the threshold are discussed. Furthermore, we introduce a new statistical tool dedicated for exploratory analysis of abnormal observations using data depth as a score. Abnormal component analysis (shortly ACA) is a method that searches a low-dimensional data representation which best visualises and explains anomalies. This low-dimensional representation not only allows to distinguish groups of anomalies better than the methods of the state of the art, but as well provides a -- linear in variables and thus easily interpretable -- explanation for anomalies. Illustrations include use-cases that underline advantageous behaviour of data depth and of the explainable anomaly detection, in various settings. </div> </div> <br> <br> <b>Jonas Wahl</b> (German Research Centre for Artificial Intelligence (DFKI))<br> September, 22th 2025, 13h-14h<br> <em>Are our DAGs correct? Recent developments in causal model evaluation</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-2" aria-expanded="false" aria-controls="collapse-past-2"> Abstract </button> <div class="collapse" id="collapse-past-2"> <div class="card card-body"> Causal graphical models are considered important tools to integrate expert knowledge into statistical data analysis. As consequence, practitioners often face the challenge to evaluate the quality of their hypothesized causal models. This issue becomes particularly salient for causal graphical models obtained through a causal discovery method in which case most or all of the available data has already been used in the discovery task. In this talk, we will review recent developments regarding the evaluation of causal models and structure learning methods. In particular, we will discuss to which degree assumption violations can be detected in causal discovery and how method testing often introduces additional assumptions that need to be weighted carefully against the assumptions of the initial learning algorithm. </div> </div> <br> <br> <b>Guilherme Holsbach Costa</b> (University of Caxias do Sul, Brazil)<br> February, 25th 2025, 14h00-15h00<br> <em>On the Super-Resolution Image Reconstruction and Phase Correlation Image Registration: methods, insights, and challenges</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-3" aria-expanded="false" aria-controls="collapse-past-3"> Abstract </button> <div class="collapse" id="collapse-past-3"> <div class="card card-body"> In this seminar, we will explore classical methods in image processing, focusing on two key topics. First, we will discuss the methodology of analytical approaches applied to the Least Mean Squares (LMS) algorithm for super-resolution image reconstruction. We will highlight how this analysis framework could also be extended to study LMS applications in image registration. In the second part, we will present an ongoing application in autonomous vehicles, where we employ Phase Correlation for image registration. This method was chosen due to its well-established position in the literature and its potential for straightforward analytical evaluation. We will talk about recent challenges we have encountered in our implementation and our ongoing efforts to overcome them. </div> </div> <br> </div> <div class="tab-pane fade " id="year-2024" role="tabpanel" aria-labelledby="tab-2024"> <br> <b>Pardis Semnani</b> (University of British Columbia, Vancouver, Canada)<br> July, 22th 2024, 14h00-15h00<br> <em>Homaloidal polynomials and Gaussian models of maximum likelihood degree one</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-1" aria-expanded="false" aria-controls="collapse-past-1"> Abstract </button> <div class="collapse" id="collapse-past-1"> <div class="card card-body"> We study the Gaussian statistical models whose log-likelihood function has a unique complex critical point, i.e., has maximum likelihood degree one. We exploit the connection developed by Améndola et. al. between the models having maximum likelihood degree one and homaloidal polynomials. We study the spanning tree generating function of a graph and show this polynomial is homaloidal when the graph is chordal. When the graph is a cycle on n vertices, n≥4, we prove the polynomial is not homaloidal, and show that the maximum likelihood degree of the resulting model is the nth Eulerian number. These results support our conjecture that the spanning tree generating function is a homaloidal polynomial if and only if the graph is chordal. We also provide an algebraic formulation for the defining equations of these models. Using existing results, we provide a computational study on constructing new families of homaloidal polynomials. In the end, we analyze the symmetric determinantal representation of such polynomials and provide an upper bound on the size of the matrices involved. </div> </div> <br> <br> <b>Helena Calatrava</b> (Northeastern University, Boston, USA)<br> July, 16th 2024, 11h00-12h00<br> <em>GNSS Signal Processing for Precise and Robust Positioning</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-2" aria-expanded="false" aria-controls="collapse-past-2"> Abstract </button> <div class="collapse" id="collapse-past-2"> <div class="card card-body"> In this talk, we will explore two methodologies designed to enhance the performance of Global Navigation Satellite Systems (GNSS): collaborative positioning techniques to improve positioning accuracy and robust signal processing to enhance resilience against jamming attacks. First, we will introduce the Massive User-Centric Single Difference (MUCSD) algorithm, which enhances GNSS accuracy through user collaboration. MUCSD leverages a network of receivers exchanging observables and noisy estimates of position and clock bias. Implemented as an iterative weighted least squares (WLS) estimator, MUCSD achieves a performance comparable to Differential GNSS (DGNSS) without the need for costly reference stations. Simulation results demonstrate that MUCSD outperforms DGNSS as the number of collaborative receivers increases, showcasing its scalability. Next, we will discuss the Robust Interference Mitigation (RIM) framework for snapshot architectures, addressing interferences such as continuous wave and chirp jamming signals. While studies on RIM typically assume the number of quantization bits allows for full signal representation, our study examines the impact of low quantization bits on baseline snapshot receiver performance in the presence of interference. Additionally, we will analyze the effect of quantization on the median absolute deviation (MAD) robust measure of statistical dispersion. By combining collaborative positioning techniques and robust interference mitigation, this talk will showcase advancements that improve GNSS precision and resilience. </div> </div> <br> <br> <b>Ivan Yakushev</b> (ENSAD, Nancy)<br> June, 19th 2024, 10h30-11h30<br> <em>Application of machine learning in visual art</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-3" aria-expanded="false" aria-controls="collapse-past-3"> Abstract </button> <div class="collapse" id="collapse-past-3"> <div class="card card-body"> This presentation will explore the current state of using local diffusion models in video and image generation, the interfaces and workflows commonly utilized in this area. In addition to highlighting the current capabilities of these models, we will also discuss their limitations and potential opportunities for future research and multidisciplinary collaboration. </div> </div> <br> <br> <b>Konstantin Usevich</b> (CRAN)<br> June, 10th 2024, 14h-15h<br> <em>Algebraic Algorithms for the ParaTuck-2 Decomposition</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-4" aria-expanded="false" aria-controls="collapse-past-4"> Abstract </button> <div class="collapse" id="collapse-past-4"> <div class="card card-body"> ParaTuck-2 decomposition (PT2D) of 3-rd order tensors is a 2-level extension of the well-known CP (canonical polyadic) decomposition (CPD). It is relevant in several applications, such as chemometrics, telecommunications, and machine learning. As shown in (Harshman, Lundy, 1996), the PT2D enjoys strong uniqueness properties (up to scaling/permutation ambiguities, similarly to the CPD). However, there are very few results on theory and algorithms for the PT2D. In particular, common strategies, such as the alternating least squares, suffer from convergence and initialization issues. We propose an algebraic algorithm for the PT2D decomposition in the case when the ParaTuck-2 ranks are smaller than the frontal dimensions of the tensor. Our approach relies only on linear algebra operations and is based on finding the kernel of a structured matrix constructed from the tensor. It refines the previously known identifiability conditions. Yet another algorithm is proposed for the symmetric case, which appears in the implicit approach to the PARAFAC-2 model. </div> </div> <br> <br> <b>Vicente Zarzoso</b> (Université Côte d'Azur, Nice)<br> April, 15th 2024, 15h-16h<br> <em>Tensor decomposition of ECG records for persistent atrial fibrillation analysis</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-5" aria-expanded="false" aria-controls="collapse-past-5"> Abstract </button> <div class="collapse" id="collapse-past-5"> <div class="card card-body"> Considered as the last great frontier of cardiac electrophysiology, atrial fibrillation (AF) is the most common sustained arrhythmia encountered in clinical practice, responsible for high hospitalization rates and a significant proportion of brain strokes in the Western world. Analyzing AF electrophysiological complexity noninvasively requires the extraction of the atrial activity (AA) signal from the electrocardiogram (ECG). To perform this task, most approaches including classical average beat subtraction need sufficiently long ECG records, thus limiting real-time analysis. Matrix factorizations can also be used for AA signal estimation by exploiting the spatial diversity of the multi-lead ECG, but require some constraints to guarantee uniqueness that may lack physiological grounds and hinder results interpretation.<br>This talk will review recent results obtained at the I3S Laboratory, UMR 7271, Université Côte d'Azur, CNRS, on tensor decompositions for noninvasive AA signal extraction in AF ECGs, which guarantee uniqueness under milder constraints on their factors. Specifically, the block term decomposition (BTD) has been shown to be particularly suitable to address this biomedical problem, as atrial and ventricular cardiac activity sources can be modeled by matrices with special structure. The structure of these matrices ensures model uniqueness while their rank is linked to signal complexity. In this framework, we have put forward the Hankel and Löwner BTD as AA extraction tools in AF ECG episodes, with validation in a population of persistent AF patients and several challenging types of ECG segments, including short beat-to-beat intervals and low-amplitude fibrillatory waves. Accurate AA extraction can be achieved from ECG segments as short as a single heartbeat. We have also developed a robust computational algorithm - the so-called alternating group lasso BTD (BTD-AGL) - to simultaneously recover the model structure (number of block terms and multilinear rank of each term) and the model factors. In addition, tensor modeling allows us to derive a novel index to quantify AF complexity nonivasively, useful to characterize stepwise catheter ablation, a first-line therapeutic option for the treatment of persistent forms of the arrhythmia. The index correlates with the expected decrease in AF complexity over ablation steps and is predictive of AF recurrence, which presents clear clinical interest. </div> </div> <br> <br> <b>Paul Catala</b> (Helmholtz Munich &amp; TUM, Germany)<br> March, 25th 2024, 14h-15h<br> <em>An Approximate Joint Diagonalization Algorithm for Off-the-Grid Sparse Recovery</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-6" aria-expanded="false" aria-controls="collapse-past-6"> Abstract </button> <div class="collapse" id="collapse-past-6"> <div class="card card-body"> Many problems in imaging and data science require to reconstruct, from partial observations, highly concentrated signals, such as pointwise sources or contour lines. This work introduces a novel algorithm for recovering measures supported on such structured domains, given a finite number of their moments. Our approach is based on the traditional singular value decomposition methodology of subspace methods, but lifts their restriction to the framework of Dirac masses, and is able to recover geometrically faithful discrete approximations of measures with density. The crucial step consists in the approximate joint diagonalization of a few non-commuting matrices, which we perform using a quasi-Newton algorithm. Experiments show that our method performs well, not only in the setting of well separated Dirac masses, as predicted by the standard theory of the truncated moment problem, but also in the case of continuous measures, which is not covered by theoretical guarantees and where usual methods empirically fail. We illustrate its applicability in optimal transport problems, where the coupling measure is often localized on the graph of some function. </div> </div> <br> <br> <b>Marc Offroy</b> (LIEC, Université de Lorraine)<br> March, 18th 2024, 10h-11h<br> <em>Extraction de signatures spectrales en imagerie Raman par des outils de chimiométrie pour la caractérisation moléculaire d’un échantillon archéologique complexe : un fragment de mosaïque datant de la période de l’oppidum (période Romaine de la seconde moitié du IIe siècle avant J.-C. à la fin du Ier siècle après J.C.)</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-7" aria-expanded="false" aria-controls="collapse-past-7"> Abstract </button> <div class="collapse" id="collapse-past-7"> <div class="card card-body"> La chimie analytique sur des artefacts archéologiques est une partie essentielle des recherches en archéologie modernes et, d'année en année, l'amélioration des instruments a permis de générer des données à une fréquence spatiale et temporelle élevée. En particulier, l'imagerie spectrale Raman peut être appliquée avec succès à la recherche en archéologie en raison de sa simplicité de mise en œuvre, afin d'étudier les sociétés humaines du passé par l'analyse de leurs vestiges provenant de fouilles. Cette technique spectrale permet d'obtenir simultanément des informations spatiales et spectrales en préservant l'intégrité de l'échantillon. Cependant, en raison de la complexité inhérente des échantillons en archéologie (ancienneté, fragilité, manque ou absence totale d'informations sur leur composition), l'interprétation chimique peut s'avérer complexe. Des problèmes spécifiques de sélectivité spectrale liés à des composés chimiques inattendus, peuvent apparaître en raison de leur condition de conservation. En outre, la détection des composés mineurs devient difficile car les composés majeurs imposent leurs contributions dans les spectres acquis. Il est donc important de développer de nouvelles approches chimiométriques afin de surmonter ces inconvénients et de découvrir ainsi toutes les informations chimiques réelles contenues dans l'ensemble des données spectrales acquises. Dans le cadre des progrès constants dans le développement d'outils mathématiques et statistiques performants, une approche chimiométrique pertinente a été introduite dans ce contexte. Cette approche vise à extraire des sources spectrales distinctes d’un jeu de données en imagerie Raman sur un échantillon archéologique : un fragment de mosaïque. L'objectif est d'extraire des informations spectrales sélectives par l'analyse du regroupement de pixels afin d'améliorer l'étape d'optimisation initiale au sein de l'algorithme MCR-ALS (Multivariate Curve Resolution and Alternating Least-Squares), une technique bien connue de démélange des signaux. Le principe sous-jacent de l'algorithme MCR-ALS est que les spectres acquis sont considérés comme des combinaisons linéaires de spectres « purs » de tous les composés chimiques individuels présents dans le système étudié. Il est parfois difficile d'obtenir les résultats souhaités par le biais de l'algorithme, en particulier si les estimations initiales des profils spectraux ou de concentrations sont inexactes en raison de signaux complexes, de bruit dans les données ou encore d'un manque de sélectivité spectrale, ce qui entraîne une déficience de rang (c'est-à-dire une mauvaise estimation du nombre total de signaux « purs »). C'est pourquoi une approche basée sur des regroupements de pixels, combiné à de multiples approches de projection orthogonale (OPA) sur les spectres, a été mise au point pour améliorer l'estimation du rang de la matrice de départ, et donc, l'étape d'initialisation de l'approche MCR-ALS avant l'optimisation. </div> </div> <br> <br> <b>Linus Bleistein</b> (Inria Paris)<br> March, 11th 2024, 14h-15h<br> <em>Dynamical Survival Analysis with Controlled Latent States</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-8" aria-expanded="false" aria-controls="collapse-past-8"> Abstract </button> <div class="collapse" id="collapse-past-8"> <div class="card card-body"> We consider the task of learning individual specific intensities of survival processes from static and longitudinal data. Modeling the intensities as solutions to non-parametric unknown differential equations allows us to provide a precise bias-variance decomposition of a signature-based estimator. This estimator yields excellent performance on a large array of both simulated and real datasets from finance, predictive maintenance and churn prediction. </div> </div> <br> <br> <b>Valentin Leplat</b> (SkolTech, Russia)<br> February, 9th 2024, 14h-15h<br> <em>Introduction to Deep Nonnegative Matrix Factorization and Stochastic Optimization with heavy-tails</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-9" aria-expanded="false" aria-controls="collapse-past-9"> Abstract </button> <div class="collapse" id="collapse-past-9"> <div class="card card-body"> Part 1: Deep Nonnegative Matrix Factorization with β-Divergences<br> Our first topic revolves around the Deep Nonnegative Matrix Factorization (deep NMF), a novel and promising facet of unsupervised learning. Deep NMF has emerged as a potent technique for extracting multi-layered features spanning various scales. However, conventional deep NMF models have primarily relied on the least squares error as their evaluation metric, which may not be the most suitable gauge for assessing the quality of approximations across diverse datasets. For data types such as audio signals and documents, β-divergences have gained recognition as a more fitting alternative. In this seminar, we present new models and algorithms that harness β-divergences to enhance deep NMF, with an emphasis on the notion of identifiability.<br><br> Part 2: Heavy-Tailed Stochastic Optimization for Deep Neural Networks<br> Our second topic concerns stochastic optimization, with a particular focus on recent discoveries concerning the nature of stochastic gradient noise in deep neural network training. Contrary to the conventional assumption of Gaussian noise, empirical evidences show that gradient noise often exhibits heavy-tailed characteristics. We introduce an efficient mechanism for optimizers to handle this noise behavior. Additionally, we showcase an extension of our recently introduced stochastic optimizer, referred to as NAG-GS, specifically tailored for the training of Vision Transformers. </div> </div> <br> <br> <b>Shuyu Dong</b> (INRIA Saclay)<br> January, 19th 2024, 15h-16h<br> <em>Low-rank matrix/tensor decomposition methods: applications to data completion and causal structure learning </em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-10" aria-expanded="false" aria-controls="collapse-past-10"> Abstract </button> <div class="collapse" id="collapse-past-10"> <div class="card card-body"> Matrix and tensor decomposition plays a crucial role in addressing various real-world problems related to topics such as statistical inference, data acquisition, and data restoration. In this talk, we start with low-rank matrix/tensor models for the data completion problem [1,2]. We tackle this problem in the framework of low-rank matrix/tensor decomposition with a least-squares model. These rank-constrained problems are known not only for their low computational complexity but also the capability of extracting the most important information in the data. We discuss a type of Riemannian gradient-based algorithms that exploit the structure of these rank-constrained models. Secondly, we present a novel application of low-rank matrix methods in the context of causal structure learning. We will show how low-rank matrix decomposition, in combination with a sparse mask operator, can be used to efficiently find directed acyclic graphs (DAGs) proximal to a given graph (with cycles). Furthermore, for learning causal DAGs from observational data, we present a sparse matrix decomposition method [4] and discuss its efficiency through experiments on synthetic and real-world data. [1] S. Dong, P.-A. Absil, and K. A. Gallivan, Riemannian gradient descent methods for graph-regularized matrix completion. Linear Algebra and its Applications 623 (2021), 193-235 [2] S. Dong, B. Gao, Y. Guan, and F. Glineur, New Riemannian preconditioned algorithms for tensor completion via polyadic decomposition, SIAM Journal on Matrix Analysis and Applications 43 (2) (2022), 840-866 [3] S. Dong and M. Sebag, From graphs to DAGs: a low-complexity model and a scalable algorithm, European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), 2022 [4] S. Dong, K. Uemura, A. Fujii, S. Chang, Y. Koyanagi, K. Maruhashi, and M. Sebag, Learning large causal structures from inverse covariance matrix via matrix decomposition, arXiv preprint arXiv:2211.14221, 2023 </div> </div> <br> </div> <div class="tab-pane fade " id="year-2023" role="tabpanel" aria-labelledby="tab-2023"> <br> <b>Xavier Luciani</b> (University of Toulon)<br> October, 5th 2023, 13h-14h<br> <em>Décomposition Canonique Polyadique et Diagonalisation Conjointe de matrices par Similitude : algorithmes et applications</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-1" aria-expanded="false" aria-controls="collapse-past-1"> Abstract </button> <div class="collapse" id="collapse-past-1"> <div class="card card-body"> En traitement du signal, la Décomposition Canonique Polyadique (DCP) consiste à décomposer un tableau multidimensionnel (appelé ici tenseur) en une combinaison multilinéaire d'un minimum de facteurs, comprenant généralement les signaux d'intérêt. Cette approche est donc couramment utilisée en séparation de sources, identification de mélange et plus généralement pour la résolution de problèmes inverses. Par ailleurs, de nombreux liens ont été établis entre la DCP et le problème de diagonalisation conjointe de matrices, que l'on retrouve également au cœur de nombreuses méthodes de séparation de sources. Nous montrerons dans la première partie de cette présentation comment réécrire la DCP sous la forme d'une Diagonalisation Conjointe de matrices par Similitude (DCS) afin d'en dériver un algorithme de calcul efficace. Nous présenterons alors plusieurs familles d'algorithmes de DCS permettant notamment de traiter le cas de signaux à valeurs complexes ou de tenir compte de contraintes de non négativité. Dans la seconde partie, nous nous placerons dans le contexte applicatif de la spectroscopie de fluorescence pour introduire un autre algorithme de DCP permettant cette fois la mise à jour des facteurs de la décomposition (et de leur nombre) au fil de l'acquisition de nouveaux signaux. Nous verrons que ces deux algorithmes très différents dans leur principe ont pour point commun une certaine résistance à la surestimation du nombre de facteurs de la DCP. Enfin nous conclurons cette présentation en évoquant nos travaux actuels en collaboration avec le CRAN et consistant à étendre les notions de DCP et de DCS aux tenseurs et matrices dont les éléments appartiennent à l'algèbre des quaternions. </div> </div> <br> <br> <b>Nuha Diab</b> (Tel Aviv University, Israel)<br> September, 26th 2023, 14h-14h40<br> <em>Optimal super-resolution of close point sources and stability of Prony's method</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-2" aria-expanded="false" aria-controls="collapse-past-2"> Abstract </button> <div class="collapse" id="collapse-past-2"> <div class="card card-body"> We consider the problem of recovering a linear combination of Dirac masses from noisy Fourier samples, also known as the problem of super-resolution. Following recent derivation of min-max bounds for this problem when some of the sources collide, we develop an optimal algorithm which provably achieves these bounds in such a challenging scenario. Our method is based on the well-known Prony's method for exponential fitting, and a novel analysis of its stability in the near-colliding regime, combined with the decimation technique for improving the conditioning of the problem. </div> </div> <br> <br> <b>Joppe De Jonghe</b> (KU Leuven, Belgium)<br> April, 6th 2023, 14h-15h<br> <em>Learning non-linearities in the two layer decoupling problem with an application to neural network compression</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-3" aria-expanded="false" aria-controls="collapse-past-3"> Abstract </button> <div class="collapse" id="collapse-past-3"> <div class="card card-body"> Methods for the decoupling of multivariate functions have been developed in order to determine the parameters and internal representations of non-linear static components in block-oriented system identification. These methods solve the single layer decoupling problem, whose solution has a natural interpretation as a neural network with a single hidden layer with flexible activation functions in the neurons. As a result, these methods have been used to compress neural (sub)networks. However, currently only compression to a single hidden layer is well understood but more complex (sub)networks may require more flexibility in the number of hidden layers. Providing compression to more than one hidden layer corresponds to approximating a solution of a multi-layer decoupling problem. In this talk I will shortly describe the single layer decoupling problem and why multi-layer decoupling is relevant, more specifically forneural network compression. Next, I will present the two layer decoupling problem as well as a solution strategy. In addition, two algorithms for approximating a solution will be discussed and described conceptually. </div> </div> <br> </div> <div class="tab-pane fade " id="year-2022" role="tabpanel" aria-labelledby="tab-2022"> <br> <b>Khazhgali Kozhasov</b> (TU Braunschweig, Germany)<br> December, 6th 2022, 14h-15h<br> <em>Real aspects of the problem of rank-one approximation</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-1" aria-expanded="false" aria-controls="collapse-past-1"> Abstract </button> <div class="collapse" id="collapse-past-1"> <div class="card card-body"> Let us consider the problem of approximating a real tensor T (of a given format) by a rank-one tensor T_1 that minimizes (the Frobenius) norm ||T-S_1|| among all rank-one tensors S_1. A tensor T_1 is, in particular, a critical point of the square of the distance function dist_T: S_1 -&gt; ||T-S_1||^2 on the manifold X of rank-one tensors. The largest possible number N of critical points of dist_T among all generic T can be interpreted as a measure of complexity of the rank-one approximation problem. I will discuss a bound on N due to Friedland and Ottaviani and will explain a technique that has been successfully used to determine a sharp bound on the number of symmetric critical points of dist_T for a symmetric tensor T. If time permits, I will discuss tensors that have worst rank-one approximation error and mention a recent result that roughly means that symmetric tensors are as far from being of rank one as general tensors. </div> </div> <br> <br> <b>Lorena León</b> (IRIT, University of Toulouse)<br> November, 17th 2022, 14h-15h<br> <em>Bayesian Multivariate Multifractal Analysis with application to Drowsiness Detection</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-2" aria-expanded="false" aria-controls="collapse-past-2"> Abstract </button> <div class="collapse" id="collapse-past-2"> <div class="card card-body"> Multifractal analysis has become a reference tool for signal and image processing. Grounded in the quantification of local regularity fluctuations, it has proven useful in an increasing range of applications, yet so far involving only univariate data (scalar valued time series or single channel images). Recently the theoretical ground for multivariate multifractal analysis has been devised, showing potential for quantifying transient higher-order dependence beyond linear correlation among collections of data. However, the accurate estimation of the parameters associated with a multivariate multifractal model remains challenging, especially for small sample size data. This work studies an original Bayesian framework for multivariate multifractal estimation, combining a novel and generic multivariate statistical model, a Whittle-based likelihood approximation and a data augmentation strategy allowing parameter separability. This careful design enables efficient estimation procedures to be constructed for two relevant choices of priors using a Gibbs sampling strategy. Monte Carlo simulations, conducted on synthetic multivariate signals and images with various sample sizes and multifractal parameter settings, demonstrate significant performance improvements over the state of the art, at only moderately larger computational cost. Moreover, we show the relevance of the proposed framework for real-world data modeling in the important application of drowsiness detection from multichannel physiological signals. </div> </div> <br> <br> <b>Jonathan Gillard</b> (Cardiff University)<br> November, 10th 2022, 14h-15h<br> <em>Low-rank methods for time series analysis.</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-3" aria-expanded="false" aria-controls="collapse-past-3"> Abstract </button> <div class="collapse" id="collapse-past-3"> <div class="card card-body"> This talk will describe some classic and recent results on low-rank methods for problems of time series analysis. The first typical and fundamental step to enable low-rank methods in this setting is to embed a time series into a Hankel matrix; low-rank approximations of this matrix which maintain the Hankel structure have meaning for classical problems such as approximation, de-noising, and forecasting. This claim will be justified in the talk and is part of a broader field known as structured low-rank approximation (SLRA). We discuss some results in SLRA before describing recent work on the nuclear norm convex relaxation of the rank minimization of Hankel matrices for forecasting, which gives rise to interesting theory and much potential for application, and can be viewed as a particular problem of matrix completion. </div> </div> <br> <br> <b>Barbara Pascal</b> (CRIStAL, Lille)<br> July, 12th 2022, 10h30-11h30<br> <em>The Kravchuk transform: a novel covariant representation for discrete signals amenable to zero-based detection tests.</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-4" aria-expanded="false" aria-controls="collapse-past-4"> Abstract </button> <div class="collapse" id="collapse-past-4"> <div class="card card-body"> Recent works in time-frequency analysis proposed to switch the focus from the maxima of the spectrogram toward its zeros, which form a random point pattern with a very stable structure. Several signal processing tasks, such as component disentanglement and signal detection procedures, have already been renewed by using modern spatial statistics onthe pattern of zeros. Tough, they require cautious choice of both the discretization strategy and the observation window in the time-frequency plane. To overcome these limitations, we propose a generalized time-frequency representation: the Kravchuk transform, especially designed for discrete signals analysis, whose phase space is the unit sphere, particularly amenable to spatial statistics. We show that it has all desired properties for signal processing, among which covariance, invertibility and symmetry, and that the point process of the zeros of the Kravchuk transform of complex white Gaussian noise coincides with the zeros of the spherical Gaussian Analytic Function. Elaborating on this theorem, we finally develop a Monte Carlo envelope test procedure for signal detection based on the spatial statistics of the zeros of the Kravchuk spectrogram. After reviewing the unorthodox path focusing on the zeros of the standard spectrogram and the associated theoretical results on the distribution of zeros in the case of white noise, I will introduce the Kravchuk transform and study the random point process of its zeros from a spatial statistics perspective. Then I will present the designed Monte Carlo envelop test, and illustrate its numerical performance in adversarial settings, with both low signal-to-noise ratio and small number of samples, and compare it to state-of-the-art zeros-based detection procedures. </div> </div> <br> <br> <b>Tulay Adali</b> (University of Maryland, Baltimore County)<br> May, 30th 2022, 13h30-14h30<br> <em>Independent Component and Vector Analyses</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-5" aria-expanded="false" aria-controls="collapse-past-5"> Abstract </button> <div class="collapse" id="collapse-past-5"> <div class="card card-body"> In many fields today, such as neuroscience, remote sensing, computational social science, and physical sciences, multiple sets of data are readily available. Matrix and tensor factorizations enable joint analysis, i.e., fusion, of these multiple datasets such that they can fully interact and inform each other while also minimizing the assumptions placed on their inherent relationships. A key advantage of these methods is the direct interpretability of their results. This talk presents an overview of models based on independent component analysis (ICA), and its generalization to multiple datasets, independent vector analysis (IVA) with examples in fusion of neuroimaging data. Relationship of IVA to other methods such as multiset canonical correlation analysis (MCCA) is discussed, and a number of important directions of research are addressed, along with the challenges. </div> </div> <br> <br> <b>Raphael Mignot</b> (IECL, Nancy)<br> May, 05th 2022, 10h30-11h30<br> <em>Barycentres de séries temporelles : une nouvelle approche basée sur la méthode de la signature</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-6" aria-expanded="false" aria-controls="collapse-past-6"> Abstract </button> <div class="collapse" id="collapse-past-6"> <div class="card card-body"> La méthode de la signature a été largement utilisée pour l'analyse des séries temporelles multivariées. Cette approche a prouvé son efficacité pour de nombreuses applications en apprentissage statistique. La définition d'une notion de barycentre dans l'espace des signatures est un premier pas prometteur permettant de développer de nouvelles extensions de l'analyse en composantes principales (ACP) ou de l'algorithme des k-moyennes aux séries temporelles. </div> </div> <br> <br> <b>Rima Khouja</b> (INRIA, Sophia Antipolis)<br> April, 28th 2022, 10h-11h<br> <em>Riemannian Newton optimization methods for the symmetric tensor approximation problem</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-7" aria-expanded="false" aria-controls="collapse-past-7"> Abstract </button> <div class="collapse" id="collapse-past-7"> <div class="card card-body"> Tensors are higher order generalization of matrices. They appear in a myriad of applications. The tensor rank decomposition is to write the tensor as a minimal sum of simple rank-1 tensors. In practice, the presence of noise in the tensor's inputs means that computing an approximated low rank decomposition is more relevant than computing the exact tensor rank decomposition. This problem is known as the low rank tensor approximation problem. In this talk, we discuss the low rank tensor approximation problem for symmetric tensors i.e. tensors with unchanged entries under any permutation of their indices. The symmetric tensors are considered with complex coefficients. We present a Riemannian optimization approach proposing Riemannian Newton and Riemannian Gauss-Newton algorithms to solve this problem. We show how the low rank symmetric tensor approximation problem can be used for tackling the problem of recovering spherical Gaussian mixture models from datasets, where the tensor is built from empirical moments of the data distribution. </div> </div> <br> </div> <div class="tab-pane fade " id="year-2021" role="tabpanel" aria-labelledby="tab-2021"> <br> <b>Clément Elvira</b> (IETR, Rennes)<br> December, 9th 2021, 10h-11h<br> <em>Safe screening: introduction and perspectives</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-1" aria-expanded="false" aria-controls="collapse-past-1"> Abstract </button> <button type="button" class="btn btn-light" onclick="window.location='/assets/seminars/2021_12_Elvira.pdf';">slides</button> <div class="collapse" id="collapse-past-1"> <div class="card card-body"> </div> </div> <br> <br> <b>Simon Barthelmé</b> (Gipsa-Lab, France)<br> November, 19th 2021, 15h-16h<br> <em>Smoothing (Large) Graph Signals using Random Forests</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-2" aria-expanded="false" aria-controls="collapse-past-2"> Abstract </button> <div class="collapse" id="collapse-past-2"> <div class="card card-body"> A natural way of denoising graph signals is to penalise local variation using the graph Laplacian. Because this has worst-case cost O(n^3) in the number of nodes, exact methods cannot be used for very large graphs. I'll show how a simple stochastic process can be used to obtain fast unbiased estimators for the smoothed signal. I'll introduce some variance reduction techniques, including a gradient-descent technique that works more generally whenever an unbiased estimator of a least-squares problem is available. Joint work with Yigit Pilavci, Nicolas Tremblay, P-O Amblard </div> </div> <br> <br> <b>Mariya Ishteva</b> (KU Leuven, Belgium)<br> October, 21th 2021, 14h30-15h<br> <em>Tensor methods with applications in system identification</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-3" aria-expanded="false" aria-controls="collapse-past-3"> Abstract </button> <button type="button" class="btn btn-light" onclick="window.location='/assets/seminars/1_Ishteva_Presentation_Mariya_2021_Nancy.pdf';">slides</button> <div class="collapse" id="collapse-past-3"> <div class="card card-body"> TBD </div> </div> <br> <br> <b>Jean-Yves Tourneret</b> (ENSEEIHT, Toulouse)<br> October, 21th 2021, 15h-15h40<br> <em>Hypersphere Fitting: Model, Algorithms and Future Work</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-4" aria-expanded="false" aria-controls="collapse-past-4"> Abstract </button> <button type="button" class="btn btn-light" onclick="window.location='/assets/seminars/2_Tourneret_Slides_Nancy_21octobre2021.pdf';">slides</button> <div class="collapse" id="collapse-past-4"> <div class="card card-body"> We will present a recent EM algorithm for hypersphere fitting based on a von Mises-Fisher prior. The algorithm achieves competitive performance compared to the state-of-the-art. In addition, it can be easily robustified to mitigate the presence of potential outliers. After presenting some results obtained with this algorithm, we will discuss some open issues related to the application to LiDAR point clouds. These open issues include the consideration of mixtures of hyperspheres, the segmentation and denoising of LiDAR point clouds and the fusion of point clouds with RGB images </div> </div> <br> <br> <b>Eric Chaumette</b> (ISAE-Supaéro, Toulouse)<br> October, 21th 2021, 15h40-16h10<br> <em>Robust Linearly Constrained Filtering and Smoothing: Results and Applications</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-5" aria-expanded="false" aria-controls="collapse-past-5"> Abstract </button> <button type="button" class="btn btn-light" onclick="window.location='/assets/seminars/3_Chaumette_Slides_Robust_LCKF_CRAN.pdf';">slides</button> <div class="collapse" id="collapse-past-5"> <div class="card card-body"> It is well-known that Wiener and Kalman filter (KF) like techniques are sensitive to misspecified covariances, uncertainties in the system matrices and parameters, filter initialization or unexpected system behaviors induced by time-varying environments, harsh propagation conditions, malicious interferences or unmodeled inputs. In this talk we introduce a possible solution to robustify these estimation techniques through linear constraints (LCs): i) we detail the linearly constrained KF (LCKF), where a set of non-stationary LCs can be set at every time step, ii) we show how to use such LCs to mitigate modeling errors in general mismatched linear discrete state-space models, and iii) we point the reader to some recent LCKF extensions (i.e., information filter, invariant filter, linear smoother and LC-extended/cubature-KF). Some applications of interest are provided to support the discussion: robust array processing, GNSS position and attitude estimation, invariant navigation and visual SLAM. </div> </div> <br> <br> <b>Radu Ranta</b> (CRAN, Université de Lorraine)<br> June, 24th 2021, 10h-11h<br> <em>Low-Rank Inverse Problems in Brain Signal Processing</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-6" aria-expanded="false" aria-controls="collapse-past-6"> Abstract </button> <div class="collapse" id="collapse-past-6"> <div class="card card-body"> The presentation will start by introducing the basics principles of biophysics allowing to model the electrophysiological brain measurements (EEG / SEEG / micro-electrodes), and more precisely the relations between the neural current sources and the electrodes. Once the signal model defined, I will briefly present some of the classical methods for solving the inverse problem of brain sources estimation (localization and activity), and I will focus next on our work on sparse approximations and low-rank (exact and approximate) source estimates. </div> </div> <br> <br> <b>Gaëtan Frusque</b> (ETH Zürich)<br> April, 16th 2021, 10h-11h<br> <em>Inférence et décomposition de graphes dynamiques en neurosciences</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-7" aria-expanded="false" aria-controls="collapse-past-7"> Abstract </button> <div class="collapse" id="collapse-past-7"> <div class="card card-body"> Dynamic graphs make it possible to understand the evolution of complex systems which evolve through time. In this thesis, we look at their applications to understand one of the most common neurological disorder in the world, affecting around 1% of the population: epilepsy. A complete and objective characterization of the patient-specific dynamic graph describing this pathology is crucial for optimal surgical treatment. First, we propose to modify a measure of functional connectivity, the Phase-Locking-Value, in order to infer robust dynamic graph from the neurophysiological signal recorded during an epileptic seizure. Constrained matrix decomposition method is applied to extract the principal features from the dynamic graph describing the pathology. Finally, a clinical study is performed to compare the obtained features from the visual interpretation of a clinician specialized in neurophysiological signal interpretation. </div> </div> <br> <br> <b>Titouan Parcollet</b> (Université d’Avignon)<br> March, 24th 2021, 14h-15h<br> <em>Should we use quaternion neural networks? Recent advances and limitations.</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-8" aria-expanded="false" aria-controls="collapse-past-8"> Abstract </button> <div class="collapse" id="collapse-past-8"> <div class="card card-body"> Real-world data used to train modern artificial neural networks reflect the complexity of the environment that we are evolving in. As a consequence, they are neither flat, nor decorrelated nor one dimensional. Instead, scientists have to deal with composed and multidimensional entities, that are characterized by multiple related components, such as color channels describing a single color pixel of an image, or the 3D coordinates of a point denoting the position of a robot. Surprisingly, recent advances on deep learning are mainly focused on developing novel architectures to extract even more relevant and robust high level representations from the input features, while the latter are still poorly considered at a lower and basic level, by being processed with one dimensional real-valued neural models. Neural networks based on complex and quaternion numbers have been used sparsely for many decades. Nonetheless, due to new statements and proofs about the benefits of these models over real-valued ones on many real-world tasks, quaternion based neural networks have been increasingly employed, and novel quaternion based architectures have been proposed. This talk will detail quaternion neural networks architectures for artificial intelligence related tasks, such as image processing, or speech recognition, by introducing first the basics of quaternion numbers, and then describing recent advances on quaternion neural networks with the quaternion convolutional (Interspeech 2018, ICASSP 2019) and recurrent neural networks (ICLR 2019, Interspeech 2020). This presentation will also show their benefits in terms of performances obtained in different tasks, as well as in terms of neural parameters required for learning. Finally, the talk will outline important future research directions to turn quaternion neural networks into a mandatory alternative to real-valued models for real-world tasks. </div> </div> <br> <br> <b>Konstantin Usevich</b> (CRAN, SiMul)<br> February, 05th 2021, 10h-11h<br> <em>Kernel matrices in the flat limit</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-9" aria-expanded="false" aria-controls="collapse-past-9"> Abstract </button> <div class="collapse" id="collapse-past-9"> <div class="card card-body"> Kernel matrices are ubiquitous in statistics and machine learning, where they occur most often as covariance matrices of Gaussian processes, in non-parametric or semi-parametric models. In approximation theory, they appear, for example, in approximation and interpolation with radial basis functions. Most of the theoretical work on kernel methods has focused on a large-n asymptotics, characterising the behaviour of kernel matrices as the amount of data increases. Fixed-sample analysis is much more difficult outside of simple cases, such as locations on a regular grid. In this talk I will describe a fixed-sample analysis that was first studied in the context of approximation theory by Fornberg &amp; Driscoll (2002), called the “flat limit”. In flat-limit asymptotics, the goal is to characterise kernel methods as the length-scale of the kernel function tends to infinity, so that kernels appear flat over the range of the data. While the resulting kernel matrix becomes singular, fascinatingly, the interpolation and regression problems remain well-defined in the limit. I the talk, I will mainly report recent results on spectral properties of kernel matrices ( https://arxiv.org/abs/1910.14067). In the flat limit, different types of kernels behave differently, and what matters most is the smoothness of kernel functions. The flat limit also highlights the close kinship between kernel methods and polynomial and spline regression. If time permits, I will discuss some implications for GP regression and Determinantal Point Processes. This is joint work with S. Barthelmé, N. Tremblay and P.-O. Amblard (GIPSA-lab, Grenoble). </div> </div> <br> <br> <b>Fateme Ghayem</b> (Gipsa-Lab, Grenoble)<br> January, 13th 2021, 10h-11h<br> <em>Optimal sensor placement for signal extraction</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-10" aria-expanded="false" aria-controls="collapse-past-10"> Abstract </button> <div class="collapse" id="collapse-past-10"> <div class="card card-body"> Many signal processing problems can be cast from a generic setting where a source signal propagates through a given environment to some sensors. Under this setting, we can be interested either in (i) estimating the source signal, or (ii) the environment, or even (iii) the resulting field of signals in some regions of the environment. In all these cases, signals are recorded by multiple sensors located at different positions. Due to price, energy, or ergonomic constraints, the number of sensors is often limited and it becomes crucial to place a few sensors at positions that contain the maximum information. This problem corresponds to optimal sensor placement and it appears in a great number of applications. The way to tackle the problem of optimal sensor placement depends on which of the three aspects mentioned above we want to address. In this talk, we focus on estimating a source signal from a set of noisy measurements collected from a limited number of sensors, and we present new criteria as well as algorithms. Specifically, our first proposed criterion maximizes the average signal to noise ratio (SNR) of the estimated signal, and we experimentally show that the performance obtained by this criterion outperforms the results obtained using classical Kriging-based methods. Since the SNR is uncertain in this context, to achieve a robust signal extraction, we propose a second placement criterion based on the maximization of the probability that the output SNR exceeds a given threshold. This criterion can be easily evaluated using a Gaussian process assumption for the signal, the noise, and the environment. Moreover, to reduce the computational complexity of the joint maximization of the criterion with respect to all sensor positions, we propose a greedy algorithm where the sensor positions are sequentially (i.e. one by one) selected. Finally, for improving the sub-optimal greedy algorithm, we present an optimization approach to locate all the sensors at once. For this purpose, we add a constraint to the problem that can control the average distances between the sensors. To solve our problem, we use an alternating optimization penalty method. </div> </div> <br> </div> <div class="tab-pane fade " id="year-2020" role="tabpanel" aria-labelledby="tab-2020"> <br> <b>Nikola Besic</b> (Centre Météorologie Radar de Météo-France, Toulouse)<br> December, 14th 2020, 14h-15h<br> <em>Mes expériences dans la télédétection radar : la Terre depuis le ciel, le ciel depuis la Terre, et comment profiter des deux ?</em><br> <button class="btn btn-primary" type="button" data-toggle="collapse" data-target="#collapse-past-1" aria-expanded="false" aria-controls="collapse-past-1"> Abstract </button> <div class="collapse" id="collapse-past-1"> <div class="card card-body"> La télédétection radar est une discipline qui repose sur le traitement du signal, d'images et de données, et sur la physique. Caractérisée par les nombreuses spécificités par rapport aux autres moyens de télédétection, cette discipline s'est montrée indispensable dans l'observation de la Terre et de l'atmosphère. De plus, il s'agit d'un domaine qui a motivé et permis de nombreux travaux de recherche concernant l'analyse statistique du signal, d'images et de données. Nikola Besic partagera avec nous certaines de ces expériences dans l'observation de la Terre depuis le ciel, et dans l'observation du ciel depuis la Terre, au moyen de radar. La première partie de son exposé adresse alors les travaux effectués sur le sujet du Radar à Synthèse d'Ouverture, satellitaire et polarimétrique, qui incluent la problématique des modèles statistiques et de la décomposition, ainsi que l'application dans le contexte des études de la cryosphère. La deuxième partie concerne plutôt l'observation de l'atmosphère avec un radar toujours polarimétrique, mais cette fois-ci terrestre, et ses efforts de trouver un compromis entre la physique et l'apprentissage automatique à partir de données, dans un contexte des méthodes de classification semi-supervisée. </div> </div> <br> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 SiMul Research group @ CRAN. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>